{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "zHBKRWL7wcDt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "x_train = torch.FloatTensor([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5],\n",
        " [1,2,5,6], [1,6,6,6], [1,7,7,7] ])\n",
        "y_train = torch.FloatTensor([ [0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0],\n",
        " [1,0,0], [1,0,0] ])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn(4, 3, requires_grad = True) # w,b 초기화\n",
        "b = torch.randn(1, 3, requires_grad = True)\n",
        "\n",
        "optimizer = torch.optim.Adam([w, b], lr = 0.1) # Optimizer 생성"
      ],
      "metadata": {
        "id": "i2DAkbW092Co"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3001) :\n",
        "  hypothesis = torch.softmax(torch.mm(x_train, w) + b, dim = 1)\n",
        "  cost = -torch.mean(torch.sum(y_train * torch.log(hypothesis), dim = 1))\n",
        "  hypothesis = (torch.mm(x_train, w) + b).softmax(dim = 1)\n",
        "  cost = -(y_train * torch.log(hypothesis)).sum(dim = 1).mean()\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 300 == 0:\n",
        "    print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDGWlqoK-WIr",
        "outputId": "9b4facf1-bb2c-4003-edf7-086ceb135dfe"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 8.774444\n",
            "epoch: 300, cost: 0.183839\n",
            "epoch: 600, cost: 0.088209\n",
            "epoch: 900, cost: 0.051452\n",
            "epoch: 1200, cost: 0.033456\n",
            "epoch: 1500, cost: 0.023295\n",
            "epoch: 1800, cost: 0.016999\n",
            "epoch: 2100, cost: 0.012830\n",
            "epoch: 2400, cost: 0.009931\n",
            "epoch: 2700, cost: 0.007837\n",
            "epoch: 3000, cost: 0.006279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w.requires_grad_(False)\n",
        "b.requires_grad_(False)\n",
        "\n",
        "x_test = torch.FloatTensor([[1,11,10,9], [1,3,4,3], [1,1,0,1]])\n",
        "# x의 값이 [1,11,10,9], [1,3,4,3], [1,1,0,1]일때\n",
        "test_all = torch.softmax(torch.mm(x_test, w) + b, dim=1)\n",
        "print(test_all)\n",
        "print(torch.argmax(test_all, dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5zxPIlqXclA",
        "outputId": "d61ba169-7bb7-480b-fdd8-5abffcac4ecc"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000e+00, 2.5464e-15, 8.8374e-33],\n",
            "        [2.3151e-01, 6.8680e-01, 8.1687e-02],\n",
            "        [5.9586e-28, 1.4646e-10, 1.0000e+00]])\n",
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F # softmax 함수 간단하게\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "lFuLCCPXYPlG"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [1,0,0],[0,1,0],[0,0,1] 대신 0,1,2사용\n",
        "\n",
        "# y_train = torch.FloatTensor([ [0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0],\n",
        "#  [1,0,0], [1,0,0] ])\n",
        "\n",
        "y_train = torch.LongTensor([2,2,2,1,1,1,0,0])"
      ],
      "metadata": {
        "id": "TSCa8xxuYvXU"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존의 w,b를 nn.Linear로 대체\n",
        "# nn.Linear =  xTw + b\n",
        "\n",
        "# 기존 코드\n",
        "# w = torch.randn(4, 3, requires_grad = True)\n",
        "# b = torch.randn(1, 3, requires_grad = True)\n",
        "# optimizer = torch.optim.Adam([w, b], lr = 0.1)\n",
        "\n",
        "model = nn.Linear(4,3)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1)"
      ],
      "metadata": {
        "id": "qHEonzBVfnkG"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3001) :\n",
        "  # hypothesis = torch.softmax(torch.mm(x_train, w) + b, dim = 1)\n",
        "  # cost = -torch.mean(torch.sum(y_train * torch.log(hypothesis), dim = 1))\n",
        "  # hypothesis = (torch.mm(x_train, w) + b).softmax(dim = 1)\n",
        "  # cost = -(y_train * torch.log(hypothesis)).sum(dim = 1).mean()\n",
        "  hypothesis = model(x_train)\n",
        "  cost = F.cross_entropy(hypothesis, y_train)\n",
        "  # 가설, 비용 함수를 제공하는 함수로 대체,  F.cross_entropy 는 softmax와 cross entropy를 합친 것\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 100 == 0:\n",
        "    print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAlt7T6iZ3J9",
        "outputId": "0f0334cc-2569-47a2-df86-a1e41aba1f82"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 4.363610\n",
            "epoch: 100, cost: 0.081484\n",
            "epoch: 200, cost: 0.042999\n",
            "epoch: 300, cost: 0.027075\n",
            "epoch: 400, cost: 0.018684\n",
            "epoch: 500, cost: 0.013724\n",
            "epoch: 600, cost: 0.010541\n",
            "epoch: 700, cost: 0.008370\n",
            "epoch: 800, cost: 0.006818\n",
            "epoch: 900, cost: 0.005667\n",
            "epoch: 1000, cost: 0.004787\n",
            "epoch: 1100, cost: 0.004099\n",
            "epoch: 1200, cost: 0.003549\n",
            "epoch: 1300, cost: 0.003101\n",
            "epoch: 1400, cost: 0.002733\n",
            "epoch: 1500, cost: 0.002424\n",
            "epoch: 1600, cost: 0.002164\n",
            "epoch: 1700, cost: 0.001942\n",
            "epoch: 1800, cost: 0.001751\n",
            "epoch: 1900, cost: 0.001586\n",
            "epoch: 2000, cost: 0.001441\n",
            "epoch: 2100, cost: 0.001315\n",
            "epoch: 2200, cost: 0.001203\n",
            "epoch: 2300, cost: 0.001103\n",
            "epoch: 2400, cost: 0.001014\n",
            "epoch: 2500, cost: 0.000935\n",
            "epoch: 2600, cost: 0.000864\n",
            "epoch: 2700, cost: 0.000799\n",
            "epoch: 2800, cost: 0.000741\n",
            "epoch: 2900, cost: 0.000688\n",
            "epoch: 3000, cost: 0.000640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Regression in Sklearn\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "x_train = np.array([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5],\n",
        " [1,2,5,6], [1,6,6,6], [1,7,7,7] ])\n",
        "# y에 0, 1, 2 등 둘 이상의 class가 존재 => softmax regression\n",
        "y_train = np.array([ 2, 2, 2, 1, 1, 1, 0, 0 ])\n",
        "logistic = LogisticRegression() # 모델 생성\n",
        "logistic.fit(x_train, y_train) # 학습\n",
        "pred = logistic.predict([[1,11,10,9], [1,3,4,3], [1,1,0,1]]) # test case (값 예측)\n",
        "print(pred) # 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNVcYyu7i32X",
        "outputId": "c5481900-ed48-4171-aa04-2c29331ae7a2"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 2 2]\n"
          ]
        }
      ]
    }
  ]
}